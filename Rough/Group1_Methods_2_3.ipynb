{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOlz2aZxRZVFUv6BocX3oEV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QcpDjOwmA9R5"},"outputs":[],"source":["#Connect to Google Drive\n","from google.colab import drive\n","import os\n","\n","#drive.mount('/content/gdrive')\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","print(os.getcwd())\n","os.chdir('gdrive/MyDrive/Deep_Learning_Project')\n","print(os.getcwd())"]},{"cell_type":"code","source":["#Required Modules\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import datetime\n","import os\n","import sklearn.preprocessing as skp\n","import collections\n","import time\n","import math\n","import matplotlib.pyplot as plt\n","import json\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers, Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Activation, Conv2D, Conv3D, MaxPool2D, MaxPool3D, AveragePooling2D, AveragePooling3D, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.layers import LocallyConnected2D, Concatenate, Reshape, ConvLSTM2D, LSTM, RNN, Bidirectional, TimeDistributed, LeakyReLU\n","from tensorflow.keras.optimizers.legacy import SGD, RMSprop, Adam\n","\n","from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\n","from tensorflow.keras.losses import BinaryCrossentropy"],"metadata":{"id":"zS_X3JxPBMTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Get Sense of Label Imbalance to Correct for\n","label_weights = {0:0, 1:0}\n","for files in sorted(os.listdir('NOAA_Event_Labels')):\n","  a = np.load('NOAA_Event_Labels/'+files, mmap_mode='r')\n","  label_weights[1] += np.sum(a)\n","  label_weights[0] += a.shape[0] * 400 - label_weights[1]\n","print(label_weights)\n","\n","numerator = sum(label_weights.values())\n","label_weights[0] = numerator/(2*label_weights[0])\n","label_weights[1] = numerator/(2*label_weights[1])\n","print(label_weights)"],"metadata":{"id":"Tb_YHKLtBMV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create Lists for Validation and Testing\n","total_files = list(zip(sorted(os.listdir('ERA_Numpy_Files')), sorted(os.listdir('NOAA_Event_Labels'))))\n","train_files = total_files[:96]\n","val_files = total_files[96:120]\n","test_files = total_files[120:]\n","#Final_Training\n","true_train = total_files[:120]\n","\n","#Quickly Geerate the Test-Labels\n","test_labelset = []\n","label_weights = {0:0, 1:0}\n","for files in test_files:\n","  a = np.load('NOAA_Event_Labels/'+files[1], mmap_mode='r')\n","  test_labelset.append(a)\n","test_labelset = np.concatenate(test_labelset, axis = 0)\n","print(test_labelset.shape)"],"metadata":{"id":"hsz8v7zCBiE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creating Generator Functions\n","def generate_batchs(files, batch_size):\n","  counter = 0\n","  while True:\n","    fname = files[counter]\n","    counter = (counter + 1) % len(files)\n","    x = np.load('ERA_Numpy_Files/'+fname[0], mmap_mode='r')\n","    y = np.load('NOAA_Event_Labels/'+fname[1], mmap_mode='r')\n","    for local_index in range(0, x.shape[0], batch_size):\n","      input_local = x[local_index:(local_index+batch_size)]\n","      output_local = y[local_index:(local_index+batch_size)]\n","      yield input_local, output_local\n","\n","def test_batchs(files, batch_size):\n","  counter = 0\n","  while True:\n","    fname = files[counter]\n","    counter += 1\n","    x = np.load('ERA_Numpy_Files/'+fname[0], mmap_mode='r')\n","    y = np.load('NOAA_Event_Labels/'+fname[1], mmap_mode='r')\n","    for local_index in range(0, x.shape[0], batch_size):\n","      input_local = x[local_index:(local_index+batch_size)]\n","      output_local = y[local_index:(local_index+batch_size)]\n","      yield input_local"],"metadata":{"id":"03V_othABiDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create Datasets From Loaders\n","batch_size = 128\n","\n","train_dataset = tf.data.Dataset.from_generator(\n","    generator=lambda: generate_batchs(files=train_files, batch_size=batch_size),\n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, 5, 20, 20, 148], [None, 20, 20, 1])\n",")\n","\n","valid_dataset = tf.data.Dataset.from_generator(\n","    generator=lambda: generate_batchs(files=val_files, batch_size=batch_size),\n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, 5, 20, 20, 148], [None, 20, 20, 1])\n",")\n","\n","test_dataset = tf.data.Dataset.from_generator(\n","    generator=lambda: test_batchs(files=test_files, batch_size=batch_size),\n","    output_types=tf.float32,\n","    output_shapes=[None, 5, 20, 20, 148]\n",")\n","\n","true_train_ds = tf.data.Dataset.from_generator(\n","    generator=lambda: generate_batchs(files=true_train, batch_size=batch_size),\n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, 5, 20, 20, 148], [None, 20, 20, 1])\n",")\n","\n","test_valid = tf.data.Dataset.from_generator(\n","    generator=lambda: generate_batchs(files=test_files, batch_size=batch_size),\n","    output_types=(tf.float32, tf.float32),\n","    output_shapes=([None, 5, 20, 20, 148], [None, 20, 20, 1])\n",")"],"metadata":{"id":"_TMYV91nB-Jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Method 2 Model: Single Convolution and Feed Forward\n","\n","model_2 = tf.keras.models.Sequential()\n","model_2.add(tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), input_shape=(5, 20, 20, 148), padding='same', activation='relu'))\n","model_2.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)))\n","model_2.add(tf.keras.layers.Flatten())\n","model_2.add(tf.keras.layers.Dense(128, activation='relu'))\n","model_2.add(tf.keras.layers.Dropout(0.5))\n","\n","# Output layer with units equal to the number of pixels\n","model_2.add(tf.keras.layers.Dense(20 * 20 * 1, activation='sigmoid'))\n","\n","# Reshape the output to match the label shape\n","model_2.add(tf.keras.layers.Reshape((20, 20, 1)))\n","\n","# Print model summary\n","model_2.summary()"],"metadata":{"id":"LPLZkB6MCGr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Validation Step for Fine Tuning\n","#Note: Due to Failing Session on Collab, process was required done in parts with resaving\n","\n","model_2.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.BinaryCrossentropy(),\n","              metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","history = model_2.fit(train_dataset, epochs=20, validation_data=valid_dataset, class_weight={0:1, 1:5000},\n","          validation_steps = len(val_files)*6, steps_per_epoch = len(train_files)*6, verbose = 1, max_queue_size = 32)\n","\n","model.save('Models_and_Histories/aratry_m1.h5')"],"metadata":{"id":"ls5_Hc-CCXua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Final Results of Validation:\n","#Weights: {0:1, 1:5000}\n","#Epochs: 6\n","\n","#Recreate Model 2\n","model_2 = tf.keras.models.Sequential()\n","model_2.add(tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), input_shape=(5, 20, 20, 148), padding='same', activation='relu'))\n","model_2.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)))\n","model_2.add(tf.keras.layers.Flatten())\n","model_2.add(tf.keras.layers.Dense(128, activation='relu'))\n","model_2.add(tf.keras.layers.Dropout(0.5))\n","\n","# Output layer with units equal to the number of pixels\n","model_2.add(tf.keras.layers.Dense(20 * 20 * 1, activation='sigmoid'))\n","\n","# Reshape the output to match the label shape\n","model_2.add(tf.keras.layers.Reshape((20, 20, 1)))\n","\n","# Print model summary\n","model_2.summary()"],"metadata":{"id":"qBnQ2P8mCrAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Retrain Model 2 under all but training data\n","model_2.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.BinaryCrossentropy(),\n","              metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","history = model_2.fit(true_train_ds, epochs=6, validation_data=test_valid, class_weight={0:1, 1:5000},\n","          validation_steps = (len(test_files) + 1)*6, steps_per_epoch = len(true_train)*6, verbose = 1, max_queue_size = 32)\n","\n","#Save model\n","model.save('Models_and_Histories/aratry_m1_final.h5')"],"metadata":{"id":"gNr4r9F9CxxP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Model 3:\n","myDO = 0.3\n","model_3 = tf.keras.models.Sequential()\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (4,4), input_shape = (5, 20, 20, 148), padding = 'same', return_sequences = True,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(BatchNormalization())\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (5,5), padding = 'same', return_sequences = True,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(BatchNormalization())\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (3,3), padding = 'same', return_sequences = True,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(BatchNormalization())\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 32, kernel_size = (1,1), padding = 'same', return_sequences = False,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(Conv2D(filters = 1, kernel_size = (1,1), activation = 'sigmoid', padding = 'same'))\n","\n","model_3.summary()"],"metadata":{"id":"lLanNJu0Df1l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Validate + Finetune\n","model_3.compile(optimizer = tf.keras.optimizers.Adam(clipnorm = 1), loss = tf.keras.losses.BinaryCrossentropy(),\n","              metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","\n","history_3 = model_3.fit(steps_per_epoch = len(train_files)*6, x = train_dataset, verbose = 1, max_queue_size = 32, epochs = 20,\n","          validation_data = valid_dataset, validation_steps = len(val_files)*6, class_weight = {0:1, 1: 5000}, shuffle = False)\n","model_3.save('Models_and_Histories/model_2.h5')"],"metadata":{"id":"IxjUIQpKEUTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Validation Fine Tuning Results:\n","#Loss Weights: {0:1, 1:5000}\n","#Epochs: 8\n","\n","#Recreate Model 3\n","myDO = 0.3\n","model_3 = tf.keras.models.Sequential()\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (4,4), input_shape = (5, 20, 20, 148), padding = 'same', return_sequences = True,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(BatchNormalization())\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (5,5), padding = 'same', return_sequences = True,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(BatchNormalization())\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 64, kernel_size = (3,3), padding = 'same', return_sequences = True,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(BatchNormalization())\n","model_3.add(tf.keras.layers.ConvLSTM2D(filters = 32, kernel_size = (1,1), padding = 'same', return_sequences = False,\n","                                       activation = 'relu', dropout = myDO, recurrent_dropout = myDO, recurrent_regularizer='l1'))\n","model_3.add(Conv2D(filters = 1, kernel_size = (1,1), activation = 'sigmoid', padding = 'same'))\n","\n","model_3.summary()"],"metadata":{"id":"VS42QFUcEfRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Retrain\n","model_3.compile(optimizer = Adam(clipnorm = 1), loss = tf.keras.losses.BinaryCrossentropy(),\n","              metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","history_3 = model_3.fit(true_train_ds, epochs=8, validation_data=test_valid, class_weight={0:1, 1:5000},\n","          validation_steps = (len(test_files) + 1)*6, steps_per_epoch = len(true_train)*6, verbose = 1, max_queue_size = 32)\n","\n","model_3.save('Models_and_Histories/model_2_final.h5')"],"metadata":{"id":"bP1Wki6cE3Sv"},"execution_count":null,"outputs":[]}]}